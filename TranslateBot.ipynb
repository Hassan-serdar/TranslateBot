{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "042986cb-bc4a-46fc-ad5b-9d85cff1acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense , Dropout\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4943b9d-3116-48a5-9b6a-d3a9cb3e5cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hassan/miniconda3/envs/tf310/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/hassan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1000000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n",
      "{'translation': {'ar': 'و هذه؟', 'en': 'And this?'}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\", \"ar-en\")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1432f90-509c-44c3-b2c1-73f2937fd394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1000000\n",
      "Average EN length: 10.161813\n",
      "Average AR length: 8.585617\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# حساب الإحصائيات الأساسية\n",
    "def avg_length(data, lang):\n",
    "    lengths = [len(x[\"translation\"][lang].split()) for x in data]\n",
    "    return sum(lengths)/len(lengths)\n",
    "\n",
    "print(\"Train samples:\", len(train_data))\n",
    "print(\"Average EN length:\", avg_length(train_data, \"en\"))\n",
    "print(\"Average AR length:\", avg_length(train_data, \"ar\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09348c9e-a307-4333-89bc-c62b9df4b625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 200000\n"
     ]
    }
   ],
   "source": [
    "import re, unicodedata, csv\n",
    "\n",
    "# Arabic normalization\n",
    "arabic_diacritics = re.compile(r\"[ًٌٍَُِّْـ]\")\n",
    "def normalize_arabic(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(arabic_diacritics, \"\", text)\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ـ+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# basic clean\n",
    "def clean_text_general(text, lang):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # remove control chars\n",
    "    text = \"\".join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
    "    # remove weird unicode (keep Arabic/Latin/numbers/punctuation)\n",
    "    if lang == \"ar\":\n",
    "        text = re.sub(r\"[^؀-ۿ0-9\\s\\.,;:\\-؟؛!\\?()\\\"'٪]\", \"\", text)\n",
    "        text = normalize_arabic(text)\n",
    "    else:\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s\\.,;:\\-!\\?()\\\"'/%@#]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_generator(dataset_split, max_len=64, min_len=3):\n",
    "    seen = set()\n",
    "    for item in dataset_split:\n",
    "        en = clean_text_general(item[\"translation\"][\"en\"], \"en\")\n",
    "        ar = clean_text_general(item[\"translation\"][\"ar\"], \"ar\")\n",
    "        if not en or not ar: \n",
    "            continue\n",
    "        if len(en.split()) < min_len or len(ar.split()) < min_len:\n",
    "            continue\n",
    "        if len(en.split()) > max_len or len(ar.split()) > max_len:\n",
    "            continue\n",
    "        key = (en, ar)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        yield {\"en\": en, \"ar\": ar}\n",
    "\n",
    "# save a subset for tokenization\n",
    "max_save = 200_000\n",
    "with open(\"train_clean_sample.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"en\", \"ar\"])\n",
    "    writer.writeheader()\n",
    "    i = 0\n",
    "    for pair in preprocess_generator(train_data, max_len=64):\n",
    "        writer.writerow(pair)\n",
    "        i += 1\n",
    "        if i >= max_save:\n",
    "            break\n",
    "print(\"Saved\", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf9c9f19-589b-4b54-b6c2-c429a365f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Read 200000 sentence pairs.\n",
      "Fitting Arabic tokenizer\n",
      "Fitting English tokenizer\n",
      "Shape of encoder_input_data: (200000, 66)\n",
      "Shape of decoder_input_data: (200000, 66)\n",
      "Shape of decoder_target_data: (200000, 66)\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"train_clean_sample.csv\"\n",
    "\n",
    "all_english_sentences = []\n",
    "all_arabic_sentences = []\n",
    "\n",
    "print(\"Reading CSV file...\")\n",
    "with open(csv_path, newline='', encoding='utf-8') as f_in:\n",
    "    reader = csv.DictReader(f_in)\n",
    "    for row in reader:\n",
    "        all_english_sentences.append(row['en'].strip())\n",
    "        all_arabic_sentences.append(row['ar'].strip())\n",
    "num_words=20000\n",
    "print(f\"Read {len(all_english_sentences)} sentence pairs.\")\n",
    "\n",
    "START_TOKEN = \"<start>\"\n",
    "END_TOKEN = \"<end>\"\n",
    "OOV_TOKEN = \"<oov>\" \n",
    "\n",
    "preprocessed_arabic_sentences = [f\"{START_TOKEN} {s} {END_TOKEN}\" for s in all_arabic_sentences]\n",
    "\n",
    "print(\"Fitting Arabic tokenizer\")\n",
    "arabic_tokenizer = Tokenizer(oov_token=OOV_TOKEN,num_words=20000) \n",
    "arabic_tokenizer.fit_on_texts(preprocessed_arabic_sentences)\n",
    "\n",
    "print(\"Fitting English tokenizer\")\n",
    "english_tokenizer = Tokenizer(num_words=num_words,oov_token=\"<OOV>\") \n",
    "english_tokenizer.fit_on_texts(all_english_sentences)\n",
    "\n",
    "\n",
    "max_len_english = max(len(s.split()) for s in all_english_sentences)\n",
    "max_len_arabic = max(len(s.split()) for s in preprocessed_arabic_sentences)\n",
    "MAX_LEN = max(max_len_english, max_len_arabic)\n",
    "\n",
    "encoder_input_data = english_tokenizer.texts_to_sequences(all_english_sentences)\n",
    "encoder_input_data = pad_sequences(encoder_input_data, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "decoder_data = arabic_tokenizer.texts_to_sequences(preprocessed_arabic_sentences)\n",
    "decoder_input_data = pad_sequences(decoder_data, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "decoder_target_data = np.zeros_like(decoder_input_data)\n",
    "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]\n",
    "\n",
    "print(f\"Shape of encoder_input_data: {encoder_input_data.shape}\")\n",
    "print(f\"Shape of decoder_input_data: {decoder_input_data.shape}\")\n",
    "print(f\"Shape of decoder_target_data: {decoder_target_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c72928c-d341-4e83-a690-686656b7bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfaaaed9-062e-4916-8340-c6a6813b11dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ encoder_embeddin… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ decoder_embeddin… │\n",
       "│                     │                   │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_output_den… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>,        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">21,181,413</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">164197</span>)           │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m2,560,000\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m2,560,000\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │    \u001b[38;5;34m131,584\u001b[0m │ encoder_embeddin… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m131,584\u001b[0m │ decoder_embeddin… │\n",
       "│                     │                   │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_output_den… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m,        │ \u001b[38;5;34m21,181,413\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │ \u001b[38;5;34m164197\u001b[0m)           │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,564,581</span> (101.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,564,581\u001b[0m (101.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,564,581</span> (101.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,564,581\u001b[0m (101.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMBEDDING_DIM=128\n",
    "LSTM_DIM=128\n",
    "encoder_inputs = Input(shape=(MAX_LEN,), name=\"encoder_input\")\n",
    "decoder_inputs = Input(shape=(MAX_LEN,), name=\"decoder_input\")\n",
    "\n",
    "\n",
    "enc_embedding_layer = Embedding(num_words, EMBEDDING_DIM, name=\"encoder_embedding\")\n",
    "enc_embedding_output = enc_embedding_layer(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(LSTM_DIM, return_state=True, name=\"encoder_lstm\")\n",
    "_, state_h, state_c = encoder_lstm(enc_embedding_output)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "dec_embedding_layer = Embedding(num_words, EMBEDDING_DIM, name=\"decoder_embedding\")\n",
    "dec_embedding_output = dec_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_DIM, return_sequences=True, return_state=False, name=\"decoder_lstm\")\n",
    "\n",
    "decoder_outputs = decoder_lstm(dec_embedding_output, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(arabic_vocab_size, activation='softmax', name=\"decoder_output_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e63f9190-6678-4cce-b35b-4ef3361c6c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m   25/40000\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56:00\u001b[0m 174ms/step - accuracy: 0.8663 - loss: 1.1732"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_enc, X_val_enc, \\\n",
    "X_train_dec, X_val_dec, \\\n",
    "y_train_dec, y_val_dec = train_test_split(encoder_input_data, \n",
    "                                          decoder_input_data, \n",
    "                                          decoder_target_data, \n",
    "                                          test_size=0.2, \n",
    "                                          random_state=42)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_enc, X_train_dec], \n",
    "    y_train_dec,           \n",
    "    batch_size=4,\n",
    "    epochs=10,                  \n",
    "    validation_data=(\n",
    "        [X_val_enc, X_val_dec], \n",
    "        y_val_dec               \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506daded-4244-4999-a52f-e9ec33e8ccbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
